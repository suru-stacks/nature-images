import csv
num_attributes = 6
a = []
print("\n The Given Training Data Set \n")
with open('/kaggle/input/enjoysport/ENJOYSPORT.csv', 'r') as csvfile:
    reader = csv.reader(csvfile)
    for row in reader:
        a.append(row)
        print(row)
print("\n The initial value of hypothesis:")
hypothesis = ['0'] * num_attributes
print(hypothesis)
for j in range(0, num_attributes):
    hypothesis[j] = a[0][j]
print("\n Find S: Finding a Maximally Specific Hypothesis\n")
for i in range(0, len(a)):
    if a[i][num_attributes] == 'yes':
        for j in range(0, num_attributes):
            if a[i][j] != hypothesis[j]:
                hypothesis[j] = '?'
            else:
                hypothesis[j] = a[i][j]
        print(" For Training instance No:{0} the hypothesis is ".format(i), hypothesis)
print("\n The Maximally Specific Hypothesis for a given Training Examples :\n")
print(hypothesis)





Practical 4:-
import pandas as pd
from sklearn import tree
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import GaussianNB
# load data from CSV
data = pd.read_csv('/kaggle/input/tennisdata-csv/tennisdata.csv')
print("THe first 5 values of data is :\n",data.head())
# obtain Train data and Train output
X = data.iloc[:,:-1]
print("\nThe First 5 values of train data is\n",X.head())
y = data.iloc[:,-1]
print("\nThe first 5 values of Train output is\n",y.head())
# Convert then in numbers
le_outlook = LabelEncoder()
X.Outlook = le_outlook.fit_transform(X.Outlook)
le_Temperature = LabelEncoder()
X.Temperature = le_Temperature.fit_transform(X.Temperature)
le_Humidity = LabelEncoder()
X.Humidity = le_Humidity.fit_transform(X.Humidity)
le_Windy = LabelEncoder()
X.Windy = le_Windy.fit_transform(X.Windy)
print("\nNow the Train data is :\n",X.head())
le_PlayTennis = LabelEncoder()
y = le_PlayTennis.fit_transform(y)
print("\nNow the Train output is\n",y)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20)
classifier = GaussianNB()
classifier.fit(X_train,y_train)
from sklearn.metrics import accuracy_score
print("Accuracy is:",accuracy_score(classifier.predict(X_test),y_test))
Practical 5:-
# import necessary libraries
import pandas as pd
from sklearn import tree
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import GaussianNB
# load data from CSV
data = pd.read_csv('/kaggle/input/documentt-csv/document.csv', names=['message', 'label'])
print("Total Instances of Dataset: ", data.shape[0])
data['labelnum'] = data.label.map({'pos': 1, 'neg': 0})
X = data.message
y = data.labelnum
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y)
from sklearn.feature_extraction.text import CountVectorizer
count_v = CountVectorizer()
Xtrain_dm = count_v.fit_transform(Xtrain)
Xtest_dm = count_v.transform(Xtest)
df = pd.DataFrame(Xtrain_dm.toarray(), columns=count_v.get_feature_names_out())
print(df[0:5])
from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB()
clf.fit(Xtrain_dm, ytrain)
pred = clf.predict(Xtest_dm)
for doc, p in zip(Xtrain, pred):
    p = 'pos' if p == 1 else 'neg'
    print("%s -> %s" % (doc, p))
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score
print('Accuracy Metrics: \n')
print('Accuracy: ', accuracy_score(ytest, pred))


Practical 7:-
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture
import pandas as pd
import numpy as np
def load_iris_data():
    iris = datasets.load_iris()
    X = pd.DataFrame(iris.data)
    X.columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width']
    y = pd.DataFrame(iris.target)
    y.columns = ['Targets']
    return X, y
def visualize_clusters(X, y, model_labels, title, colormap):
    plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[model_labels], s=40)
    plt.title(title)
    plt.xlabel('Petal Length')
    plt.ylabel('Petal Width')
def main():
    X, y = load_iris_data()
    kmeans_model = KMeans(n_clusters=3)
    kmeans_model.fit(X)
    scaler = StandardScaler()
    xs = scaler.fit_transform(X)
    gmm = GaussianMixture(n_components=3)
    gmm.fit(xs)
    plt.figure(figsize=(14, 7))
    colormap = np.array(['red', 'lime', 'black'])
    plt.subplot(1, 3, 1)
    plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[y.Targets], s=40)
    plt.title('Real Clusters')
    plt.xlabel('Petal Length')
    plt.ylabel('Petal Width')
    plt.subplot(1, 3, 2)
    visualize_clusters(X, y, kmeans_model.labels_, 'K-Means Clustering', colormap)
    plt.subplot(1, 3, 3)
    visualize_clusters(X, y, gmm.predict(xs), 'GMM Clustering', colormap)
    plt.show()
    print('Observation: The GMM using EM algorithm-based clustering matched the true labels more closely than KMeans.')
if __name__ == "__main__":
    main()


Practical 8:-

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
import numpy as np
irisData = load_iris()
X = irisData.data
y = irisData.target
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)
knn= KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train,y_train)
print(knn.predict(X_test))
print(knn.score(X_test,y_test))
neighbours = np.arange(1,9)
train_accuracy = np.empty(len(neighbours))
test_accuracy = np.empty(len(neighbours))
for i,k in enumerate(neighbours):
  knn = KNeighborsClassifier(n_neighbors=k)
  knn.fit(X_train,y_train)
train_accuracy[i] = knn.score(X_train,y_train)
test_accuracy[i] = knn.score(X_test,y_test)
plt.plot(neighbours,test_accuracy,label='Testing dataset Accuracy')
plt.plot(neighbours,train_accuracy,label='Training dataset Accuracy')
plt.legend()
plt.xlabel('n_neighbors')
plt.ylabel('Accuracy')
plt.show()
